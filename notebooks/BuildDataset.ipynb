{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17712 17712 True\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data'\n",
    "images_dir = pathlib.Path(f'{data_path}/images')\n",
    "image_count = len(list(images_dir.glob('*/*.jpg')))\n",
    "masks_dir = pathlib.Path(f'{data_path}/images_masks')\n",
    "masks_count = len(list(masks_dir.glob('*.jpg')))\n",
    "\n",
    "print(image_count, masks_count, image_count == masks_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(images_dir/'*/*'), shuffle=False)\n",
    "list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'../data/images/good/71f2ce9ab_5.jpg'\n",
      "b'../data/images/good/112942aed_1.jpg'\n",
      "b'../data/images/bad/a16672b15_6.jpg'\n",
      "b'../data/images/good/046c35525_6.jpg'\n",
      "b'../data/images/good/8c3539cdd_16.jpg'\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10627 2125 1417\n"
     ]
    }
   ],
   "source": [
    "network_split = 0.6\n",
    "validation_split = 0.2\n",
    "ae_size = int(image_count * network_split)\n",
    "ae_val_size = int(ae_size * validation_split)\n",
    "conv_val_size = int((image_count-ae_size) * validation_split)\n",
    "print(ae_size, ae_val_size, conv_val_size)\n",
    "conv_ds = list_ds.skip(ae_size)\n",
    "conv_train_ds = conv_ds.skip(conv_val_size)\n",
    "conv_val_ds = conv_ds.take(conv_val_size)\n",
    "ae_ds = list_ds.take(ae_size)\n",
    "ae_train_ds = ae_ds.skip(ae_val_size)\n",
    "ae_val_ds = ae_ds.take(ae_val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17712\n",
      "7085\n",
      "5668\n",
      "1417\n",
      "10627\n",
      "8502\n",
      "2125\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.experimental.cardinality(list_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(conv_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(conv_train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(conv_val_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(ae_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(ae_train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(ae_val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad' 'good']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted([item.name for item in images_dir.glob('*') if item.name != \".gitkeep\"]))\n",
    "print(class_names)\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    one_hot = parts[-2] == class_names\n",
    "    # Integer encode the label\n",
    "    return tf.argmax(one_hot)\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size\n",
    "    return tf.image.resize(img, [256, 256])\n",
    "\n",
    "def process_path_ae(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, img\n",
    "\n",
    "def process_path_conv(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    label = get_label(file_path)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "conv_train_ds = conv_train_ds.map(process_path_conv, num_parallel_calls=AUTOTUNE)\n",
    "conv_val_ds = conv_val_ds.map(process_path_conv, num_parallel_calls=AUTOTUNE)\n",
    "ae_train_ds = ae_train_ds.map(process_path_ae, num_parallel_calls=AUTOTUNE)\n",
    "ae_val_ds = ae_val_ds.map(process_path_ae, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "conv_train_ds = configure_for_performance(conv_train_ds) \n",
    "conv_val_ds = configure_for_performance(conv_val_ds) \n",
    "ae_train_ds = configure_for_performance(ae_train_ds) \n",
    "ae_val_ds = configure_for_performance(ae_val_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(ds, name):\n",
    "    out_path = os.path.join(data_path, name)\n",
    "    if os.path.exists(out_path):\n",
    "        raise Exception('Path exists already')\n",
    "    tf.data.experimental.save(ds, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_train_ds = save(conv_train_ds, 'conv_train_ds') \n",
    "conv_val_ds = save(conv_val_ds, 'conv_val_ds') \n",
    "ae_train_ds = save(ae_train_ds, 'ae_train_ds') \n",
    "ae_val_ds = save(ae_val_ds, 'ae_val_ds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
